{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model-training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBf9FoVO5gY",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ypbH-_VO5gZ",
        "colab_type": "text"
      },
      "source": [
        "## Create Filesystem\n",
        "This notebook is primarily meant to be executed in Colab as a computational backend. If you want to run on your own hardware with data, you need to set `data_dir` and `ALLOW_IO`\n",
        "\n",
        "This notebook viewable directly on Colab from [https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb](https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb) (it is a mirror of github). But if it has moved branches or you are looking at a past commit, look at the [Google instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on where to find this file.\n",
        "\n",
        "The workflow is:\n",
        " - Data stored in (my personal/private) Google Drive\n",
        " - Utilities/library files (for importing) on github, edited on local hardware and pushed to github.\n",
        " - Notebook hosted on github, edited both in Colab or locally (depending on the relative value of having a GPU attached versus being able to use regular Jupyter keyboard shortcuts/a superior interface)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RY-nqnzUzB5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "216818da-0013-4e05-e665-bef6526787a4"
      },
      "source": [
        "# Attempt Colab setup if on Colab\n",
        "try:\n",
        "  import google.colab\n",
        "except:\n",
        "  ALLOW_IO = False\n",
        "else:\n",
        "  # Mount Google Drive at data_dir\n",
        "  #  (for data)\n",
        "  from google.colab import drive\n",
        "  from os.path import join\n",
        "  ROOT = '/content/drive'\n",
        "  DATA = 'My Drive/phutball'\n",
        "  drive.mount(ROOT)\n",
        "  ALLOW_IO = True\n",
        "  data_dir = join(ROOT, DATA)\n",
        "  !mkdir \"{data_dir}\"     # in case we haven't created it already   \n",
        "\n",
        "  # Pull in code from github\n",
        "  %cd /content\n",
        "  github_repo = 'https://github.com/rcharan/phutball'\n",
        "  !git clone -b rl {github_repo}\n",
        "  %cd /content/phutball\n",
        "  \n",
        "  # Point python to code base\n",
        "  import sys\n",
        "  sys.path.append('/content/phutball/pytorch-implementation')\n",
        "\n",
        "  # Updater for library functions changed on local hardware and pushed to github\n",
        "  #  (circuitous, I know)\n",
        "  def update_repo():\n",
        "    !git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory â€˜/content/drive/My Drive/phutballâ€™: File exists\n",
            "/content\n",
            "Cloning into 'phutball'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 1400 (delta 5), reused 14 (delta 5), pack-reused 1382\u001b[K\n",
            "Receiving objects: 100% (1400/1400), 6.32 MiB | 9.84 MiB/s, done.\n",
            "Resolving deltas: 100% (833/833), done.\n",
            "/content/phutball\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1C55w-O5gg",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbaorYhzwUze",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "# Codebase\n",
        "from lib.model_v1          import TDConway\n",
        "from lib.moves             import create_placement_getter, get_jumps\n",
        "from lib.utilities         import config, lfilter\n",
        "from lib.testing_utilities import create_state, visualize_state, boards\n",
        "from lib.timer             import Timer\n",
        "from lib.moves             import END_LOC, COL, CHAIN\n",
        "\n",
        "# Graphics for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "plt.ioff()\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "# Tensorflow solely for the Progress Bar (totally worth it)\n",
        "from tensorflow.keras.utils import Progbar as ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOk4kKeOO5gm",
        "colab_type": "text"
      },
      "source": [
        "## Device Management Utilities\n",
        "Setup for GPU, CPU, or (not working well/fully implemented) TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ETHYgbO5gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b7f0e462-54f1-4350-e3b9-9855148a2bf1"
      },
      "source": [
        "use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "  # Install PyTorch/XLA\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  \n",
        "  # Set the device\n",
        "  device = xm.xla_device()\n",
        "  \n",
        "  # Memory inspection\n",
        "  def print_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def print_max_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def garbage_collect():\n",
        "    gc.collect() # No TPU specific implementation yet\n",
        "    \n",
        "elif torch.cuda.is_available():\n",
        "  # Set the device\n",
        "  device = torch.device('cuda')\n",
        "  \n",
        "  # Echo GPU info\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  print(gpu_info)\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage_cuda     as print_memory_usage,\n",
        "    print_max_memory_usage_cuda as print_max_memory_usage,\n",
        "    garbage_collect_cuda        as garbage_collect\n",
        "  )\n",
        "\n",
        "else:\n",
        "  # Set the device to CPU\n",
        "  device = torch.device('cpu')\n",
        "  \n",
        "  # Echo RAM info\n",
        "  from psutil import virtual_memory\n",
        "  from lib.memory import format_bytes\n",
        "  ram = virtual_memory().total\n",
        "  print(format_bytes(ram), 'available memory on CPU-based runtime')\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage, \n",
        "    print_max_memory_usage,\n",
        "    garbage_collect\n",
        "  )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 14 03:31:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    28W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE3ErAdVO5gt",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kl9_Q1WNwUz8",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "MAX_JUMPS = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmRqSIoWO5gy",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Tools\n",
        "These tools will be moved to .py files in the lib folder when stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZnQLuYPO5gy",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rkzXSlsbwU0C",
        "colab": {}
      },
      "source": [
        "class AlternatingTDLambda(Optimizer):\n",
        "  '''Implements tracing and updates for the TD(Î») algorithm.\n",
        "  \n",
        "  For details see Sutton and Barto, Reinforcement Learning 2ed,\n",
        "  Chaper 12 Section 2.\n",
        "  \n",
        "  Modifications:\n",
        "    (1) the algorithm is modified to compute traces\n",
        "        one step early (when the graph is available). Hence, trace must be\n",
        "        initialized with update_trace before calling step.\n",
        "        \n",
        "    (2) Because the board switches sides each turn, eligibility trace updates\n",
        "        must be of the form z_t+1 <-  -Î»z_t + ð¯v\n",
        "  \n",
        "  You may find the following greeks a helpful reference:\n",
        "  alpha : Learning Rate\n",
        "  lambda: Exponential Decay parameter for the eligibility trace\n",
        "          (which is essentially momentum but with different\n",
        "           interpertation and is occasionally zeroed out).\n",
        "  delta : Temporal difference (TD) i.e. the difference between the estimated\n",
        "          value of a step and the realized value upon the best move (based\n",
        "          on further estimation of course).\n",
        "  '''\n",
        "  def __init__(self, parameters, alpha, lamda):\n",
        "    self.alpha = alpha\n",
        "    self.lamda = lamda # Note alternate spelling (I didn't make it up!)\n",
        "    defaults   = dict(alpha = alpha, lamda = lamda)\n",
        "\n",
        "    super(AlternatingTDLambda, self).__init__(parameters, defaults)\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def step(self, delta, update_trace = True):\n",
        "    '''Performs a single optimization step updating the trace *afterwards*\n",
        "    \n",
        "    update_trace must be called before first step to initialize the trace.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    \n",
        "    delta: Difference between estimated value of move at time t and realized\n",
        "           value after moving and going to time t+1\n",
        "    '''\n",
        "    \n",
        "    for group in self.param_groups:\n",
        "      alpha = group['alpha']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          raise RuntimeError('Traces must be initialized before calling step')\n",
        "          \n",
        "        trace = state['trace']\n",
        "        \n",
        "        p.add_(alpha * delta * trace) # Note gradient *ascent* in reinforcement learning\n",
        "    \n",
        "    if update_trace:\n",
        "      self._update_trace()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def _update_trace(self):\n",
        "    '''Updates the trace based on the gradients.\n",
        "    \n",
        "    This also is the only way to initialize the traces.\n",
        "    It must be called after evaluating the starting position\n",
        "    and backprop at the beginning of the game, usually from\n",
        "    the optimizers restart method.\n",
        "    '''\n",
        "    for group in self.param_groups:\n",
        "      lamda = group['lamda']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        # Initialize to zero if necessary\n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          state['trace'] = torch.zeros_like(p)\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.mul_(-lamda).add_(p.grad)\n",
        "      \n",
        "  @torch.no_grad()\n",
        "  def _zero_trace(self):\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state['p']\n",
        "        if 'trace' not in state:\n",
        "          continue\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.zero_()\n",
        "    \n",
        "  def restart(self, score):\n",
        "    '''Call to restart the trace\n",
        "    \n",
        "    Returns score.item() as a convenience\n",
        "    '''\n",
        "    self._zero_trace()\n",
        "    score.backward()\n",
        "    self._update_trace()\n",
        "    self.zero_grad()\n",
        "    return score.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-RCMAEuO5g2",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1CAfi-4wU0H",
        "colab": {}
      },
      "source": [
        "def training_loop(optimizer, num_games, off_policy = lambda _ : None):\n",
        "  \n",
        "  initial_state = create_state('H10').to(device)\n",
        "  for i in range(num_games):\n",
        "    print(f'\\nPlaying game {i+1} of {num_games}:')\n",
        "    game_loop(initial_state, model, optimizer, off_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62GkFibrwU0Q",
        "colab": {}
      },
      "source": [
        "def game_loop(initial_state, model, optimizer, off_policy):\n",
        "  '''Training loop that plays one game'''\n",
        "  # Just in case\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # Initialization\n",
        "  state    = initial_state\n",
        "  score, _ = model(state.unsqueeze(0))\n",
        "  v_t      = optimizer.restart(score)\n",
        "  \n",
        "  # Progress Bar\n",
        "  bar      = ProgressBar(284)\n",
        "  move_num = 1\n",
        "  \n",
        "  while True:\n",
        "    # Determine the next move\n",
        "    game_over, moved_off_policy, new_state, score = \\\n",
        "      get_next_move_training(state, off_policy = off_policy)\n",
        "    \n",
        "    if game_over:      \n",
        "      delta = 1 - v_t\n",
        "      optimizer.step(delta, update_trace = False)\n",
        "      \n",
        "      # Terminate the progress bar\n",
        "      bar.target = move_num\n",
        "      bar.update(move_num)\n",
        "\n",
        "      break\n",
        "    \n",
        "    elif moved_off_policy:\n",
        "      # Equivalent to starting a new game\n",
        "      v_t = optimizer.restart(score)\n",
        "      \n",
        "    else:\n",
        "      score.backward()\n",
        "      delta = (1 - score) - v_t\n",
        "      optimizer.step(delta)\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      v_t   = score.item()\n",
        "      state = new_state\n",
        "      \n",
        "    # Progress bar\n",
        "    move_num += 1\n",
        "    if move_num >= bar.target * 0.9:\n",
        "      bar.target += bar.target // 10 + 1\n",
        "    \n",
        "    bar.update(move_num)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_Kl9lNTwU0T",
        "colab": {}
      },
      "source": [
        "def get_next_move_training(curr_state, off_policy = lambda _ : None, profile = 0):\n",
        "  '''Get the next move for the bot\n",
        "  \n",
        "  Gets the next move for the bot with computations and\n",
        "  return value suitable for training only\n",
        "  (i.e. gradients are taken)\n",
        "  \n",
        "  If off_policy is not None, the off_policy move is\n",
        "  selected instead.\n",
        "  \n",
        "  Gradients with respect to the value-function applied\n",
        "  at the best move are accumlated and availabe to the caller\n",
        "  \n",
        "  Inputs\n",
        "  ------\n",
        "  \n",
        "  curr_state: binary tensor of shape (channels, rows, cols)\n",
        "              reprenting the game state\n",
        "              \n",
        "  off_policy: callable with signature\n",
        "              off_policy(num_available_moves: int) returning\n",
        "              either None or the index of the move desired.\n",
        "              If the return value is not None AND the bot \n",
        "              cannot otherwise win on that move, then that\n",
        "              move is made.\n",
        "              \n",
        "  profile   : verbosity, will determine whether to time and\n",
        "              display memory information\n",
        "              \n",
        "  Outputs\n",
        "  -------\n",
        "  game_over  : boolean. Whether the bot can (and does) win\n",
        "             on this move. The bot *always* plays a\n",
        "             win-in-one move when it is available, regardless\n",
        "             of the off-policy argument.\n",
        "             \n",
        "  off_policy : boolean. Whether an off-policy move was made.\n",
        "             OR: value is None if game is over\n",
        "  \n",
        "  new_state  : a binary tensor of same shape as curr_input\n",
        "             representing the new state of the game after\n",
        "             the bot moves AND the board is flipped around\n",
        "             to present it from opponents view. OR: value\n",
        "             is None if game is over.\n",
        "               \n",
        "  value      : value of the value-function applied to new_state.\n",
        "             OR: value is None if the game is over.\n",
        "  '''\n",
        "  # Compute the placements\n",
        "  placements = get_placements(curr_state)\n",
        "\n",
        "  # Compute the jumps\n",
        "  jumps = get_jumps(curr_state, MAX_JUMPS)\n",
        "\n",
        "  # Deal with special cases/win condition for the jump\n",
        "  \n",
        "  # No jumps to worry about\n",
        "  if len(jumps) == 0:\n",
        "    moves = placements\n",
        "  \n",
        "  # Win condition\n",
        "  elif (\n",
        "    len(jumps) == 1 and\n",
        "    jumps[0][CHAIN][END_LOC][COL] in [config.cols, config.cols-1]):\n",
        "    return True, None, None, None # The game is over!\n",
        "  \n",
        "  # Regular jump evaluation\n",
        "  else:\n",
        "    # Retain only the final state\n",
        "    jumps = [jump_data[0] for jump_data in jumps]\n",
        "    jumps = torch.tensor(jumps, dtype = torch.bool, device = device)\n",
        "    moves = torch.cat([placements, jumps])\n",
        "    \n",
        "  # Turn the board around to represent the opponent's view\n",
        "  moves = torch.flip(moves, [-1])\n",
        "  \n",
        "  # Either make an off policy move, or evaluate the value-function\n",
        "  #  to determine the policy\n",
        "  off_policy_move = off_policy(moves.shape[0])\n",
        "  if off_policy_move is not None:\n",
        "    move     = moves[off_policy_move].unsqueeze(0)\n",
        "    score, _ = model(move)\n",
        "    return False, True, move, score\n",
        "  \n",
        "  # Batch the moves\n",
        "  batches = torch.split(moves, BATCH_SIZE)\n",
        "  \n",
        "  # We only need to differentiate the best score\n",
        "  #  track which one that is.\n",
        "  best_score = None\n",
        "  best_index = None\n",
        "  curr_index = 0\n",
        "\n",
        "  if profile:\n",
        "    timer = Timer()\n",
        "    print(f'Starting forward pass')\n",
        "    print_memory_usage()\n",
        "    batch_num = 0\n",
        "    \n",
        "  for batch in batches:\n",
        "\n",
        "    \n",
        "    # Run the model\n",
        "    score, index = model(batch)\n",
        "\n",
        "    # Update running tally of best score\n",
        "    #  Old best scores should have their graphs\n",
        "    #  destroyed\n",
        "\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_index = curr_index + index\n",
        "\n",
        "    # Keep track of how many indices we've traversed to \n",
        "    #  get best_index correct\n",
        "    curr_index += batch.shape[0]\n",
        "    \n",
        "    \n",
        "    if profile:\n",
        "      print('')\n",
        "      batch_num += 1\n",
        "      print(f'Completed batch {batch_num} after a total of {timer}')\n",
        "      if profile > 1:\n",
        "        print_memory_usage()\n",
        "\n",
        "  if profile > 1:\n",
        "    print_max_memory_usage()\n",
        "  \n",
        "  # Return\n",
        "  return False, False, moves[best_index], best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4-LF0-RO5hA",
        "colab_type": "text"
      },
      "source": [
        "# Off-policy policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n316gx2nO5hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedy:\n",
        "  \n",
        "  def __init__(self, epsilon):\n",
        "    self.epsilon = epsilon\n",
        "  \n",
        "  def __call__(self, num_options):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      return np.random.randint(0, num_options)\n",
        "    else:\n",
        "      return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxB31wNMO5hG",
        "colab_type": "text"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiYyY9KPXIE",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBpw7Ul8wUzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "ba836bab-56de-42d6-a14e-73f760977411"
      },
      "source": [
        "get_placements = create_placement_getter(device)\n",
        "epsilon_greedy = EpsilonGreedy(0.1)\n",
        "\n",
        "model = TDConway(config).to(device)\n",
        "optimizer = AlternatingTDLambda(model.parameters(), 0.01, 0.9)\n",
        "\n",
        "initial_state = create_state('H10').to(device)\n",
        "visualize_state(initial_state)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlOE_I84O5hD",
        "colab_type": "text"
      },
      "source": [
        "## Profile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Etz9nnawU0i",
        "outputId": "32266ba3-9ee2-4aae-e629-09c278097c01",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # Blazing fast on a P100!\n",
        "# %%prun\n",
        "# get_next_move_training(initial_state);\n",
        "# _ = _"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B20z3tcFO5hK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9aca3080-c20d-4ea1-cb88-d760dcc74a89"
      },
      "source": [
        "# # Decently fast. A pace of 10000 games per day\n",
        "# %%prun\n",
        "# training_loop(optimizer, 1, off_policy = epsilon_greedy)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Playing game 1 of 1:\n",
            "152/152 [==============================] - 13s 84ms/step\n",
            " "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqLsixqoP3kV",
        "colab_type": "text"
      },
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNxzfZaQyTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a3b258b-f9f9-4fa1-ad35-af87ee647a47"
      },
      "source": [
        "training_loop(optimizer, 100, off_policy = epsilon_greedy)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Playing game 1 of 100:\n",
            "128/128 [==============================] - 11s 86ms/step\n",
            "\n",
            "Playing game 2 of 100:\n",
            "59/59 [==============================] - 5s 85ms/step\n",
            "\n",
            "Playing game 3 of 100:\n",
            "74/74 [==============================] - 6s 81ms/step\n",
            "\n",
            "Playing game 4 of 100:\n",
            "125/125 [==============================] - 11s 87ms/step\n",
            "\n",
            "Playing game 5 of 100:\n",
            "88/88 [==============================] - 8s 92ms/step\n",
            "\n",
            "Playing game 6 of 100:\n",
            "134/134 [==============================] - 12s 86ms/step\n",
            "\n",
            "Playing game 7 of 100:\n",
            "86/86 [==============================] - 8s 89ms/step\n",
            "\n",
            "Playing game 8 of 100:\n",
            "187/187 [==============================] - 15s 79ms/step\n",
            "\n",
            "Playing game 9 of 100:\n",
            "225/225 [==============================] - 18s 79ms/step\n",
            "\n",
            "Playing game 10 of 100:\n",
            "74/74 [==============================] - 7s 93ms/step\n",
            "\n",
            "Playing game 11 of 100:\n",
            "83/83 [==============================] - 8s 91ms/step\n",
            "\n",
            "Playing game 12 of 100:\n",
            "94/94 [==============================] - 8s 89ms/step\n",
            "\n",
            "Playing game 13 of 100:\n",
            "69/69 [==============================] - 6s 91ms/step\n",
            "\n",
            "Playing game 14 of 100:\n",
            "107/107 [==============================] - 9s 88ms/step\n",
            "\n",
            "Playing game 15 of 100:\n",
            "112/112 [==============================] - 10s 87ms/step\n",
            "\n",
            "Playing game 16 of 100:\n",
            "85/85 [==============================] - 8s 94ms/step\n",
            "\n",
            "Playing game 17 of 100:\n",
            "76/76 [==============================] - 7s 91ms/step\n",
            "\n",
            "Playing game 18 of 100:\n",
            "131/131 [==============================] - 12s 90ms/step\n",
            "\n",
            "Playing game 19 of 100:\n",
            "138/138 [==============================] - 12s 88ms/step\n",
            "\n",
            "Playing game 20 of 100:\n",
            "77/77 [==============================] - 7s 91ms/step\n",
            "\n",
            "Playing game 21 of 100:\n",
            "117/117 [==============================] - 10s 84ms/step\n",
            "\n",
            "Playing game 22 of 100:\n",
            "60/60 [==============================] - 6s 94ms/step\n",
            "\n",
            "Playing game 23 of 100:\n",
            "48/48 [==============================] - 4s 87ms/step\n",
            "\n",
            "Playing game 24 of 100:\n",
            "71/71 [==============================] - 6s 91ms/step\n",
            "\n",
            "Playing game 25 of 100:\n",
            "91/91 [==============================] - 8s 87ms/step\n",
            "\n",
            "Playing game 26 of 100:\n",
            "86/86 [==============================] - 7s 86ms/step\n",
            "\n",
            "Playing game 27 of 100:\n",
            "88/88 [==============================] - 8s 92ms/step\n",
            "\n",
            "Playing game 28 of 100:\n",
            "101/101 [==============================] - 9s 88ms/step\n",
            "\n",
            "Playing game 29 of 100:\n",
            "118/118 [==============================] - 11s 90ms/step\n",
            "\n",
            "Playing game 30 of 100:\n",
            "108/108 [==============================] - 9s 86ms/step\n",
            "\n",
            "Playing game 31 of 100:\n",
            "84/84 [==============================] - 8s 95ms/step\n",
            "\n",
            "Playing game 32 of 100:\n",
            "107/284 [==========>...................] - ETA: 15s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-68c4a3f127fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-89beacbde1ce>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(optimizer, num_games, off_policy)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nPlaying game {i+1} of {num_games}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgame_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-b5d49f81e97a>\u001b[0m in \u001b[0;36mgame_loop\u001b[0;34m(initial_state, model, optimizer, off_policy)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Determine the next move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgame_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoved_off_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m       \u001b[0mget_next_move_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-b73c9def69cb>\u001b[0m in \u001b[0;36mget_next_move_training\u001b[0;34m(curr_state, off_policy, profile)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m# Compute the jumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mjumps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_jumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_JUMPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;31m# Deal with special cases/win condition for the jump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/moves.py\u001b[0m in \u001b[0;36mget_jumps\u001b[0;34m(curr_state, max_jumps)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmax_jumps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjumps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_jumps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprioritize_jumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjumps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_jumps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/moves.py\u001b[0m in \u001b[0;36mprioritize_jumps\u001b[0;34m(jumps, max_jumps)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m   \u001b[0mcol_ending_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_col_endings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjump_chains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0mcutoff_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_cutoff_to_take\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cutoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_ending_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/moves.py\u001b[0m in \u001b[0;36mcount_col_endings\u001b[0;34m(jump_chains)\u001b[0m\n\u001b[1;32m    407\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mjump_chain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjump_chains\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mend_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjump_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEND_LOC\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mvalue_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHORM9o9TCgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(fname, model, optimizer):\n",
        "  state_dict = {\n",
        "      'model' : model.state_dict(),\n",
        "      'optim' : optimizer.state_dict()\n",
        "  }\n",
        "  torch.save(state_dict, f'{data_dir}/{fname}.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFxrcCKURwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save('v0.1.1-31', model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peBl22jDUjPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}