{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model-training.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sOBf9FoVO5gY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ypbH-_VO5gZ"
      },
      "source": [
        "## Create Filesystem\n",
        "This notebook is primarily meant to be executed in Colab as a computational backend. If you want to run on your own hardware with data, you need to set `data_dir` and `ALLOW_IO`\n",
        "\n",
        "This notebook viewable directly on Colab from [https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb](https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb) (it is a mirror of github). But if it has moved branches or you are looking at a past commit, look at the [Google instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on where to find this file.\n",
        "\n",
        "The workflow is:\n",
        " - Data stored in (my personal/private) Google Drive\n",
        " - Utilities/library files (for importing) on github, edited on local hardware and pushed to github.\n",
        " - Notebook hosted on github, edited both in Colab or locally (depending on the relative value of having a GPU attached versus being able to use regular Jupyter keyboard shortcuts/a superior interface)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RY-nqnzUzB5k",
        "outputId": "319a4ab6-f86d-4663-ab4d-e8acb27481fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Attempt Colab setup if on Colab\n",
        "try:\n",
        "  import google.colab\n",
        "except:\n",
        "  ALLOW_IO = False\n",
        "else:\n",
        "  # Mount Google Drive at data_dir\n",
        "  #  (for data)\n",
        "  from google.colab import drive\n",
        "  from os.path import join\n",
        "  ROOT = '/content/drive'\n",
        "  DATA = 'My Drive/phutball'\n",
        "  drive.mount(ROOT)\n",
        "  ALLOW_IO = True\n",
        "  data_dir = join(ROOT, DATA)\n",
        "  !mkdir \"{data_dir}\"     # in case we haven't created it already   \n",
        "\n",
        "  # Pull in code from github\n",
        "  %cd /content\n",
        "  github_repo = 'https://github.com/rcharan/phutball'\n",
        "  !git clone -b rl {github_repo}\n",
        "  %cd /content/phutball\n",
        "  \n",
        "  # Point python to code base\n",
        "  import sys\n",
        "  sys.path.append('/content/phutball/pytorch-implementation')\n",
        "\n",
        "  # Updater for library functions changed on local hardware and pushed to github\n",
        "  #  (circuitous, I know)\n",
        "  def update_repo():\n",
        "    !git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory â€˜/content/drive/My Drive/phutballâ€™: File exists\n",
            "/content\n",
            "Cloning into 'phutball'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 1572 (delta 121), reused 107 (delta 53), pack-reused 1382\u001b[K\n",
            "Receiving objects: 100% (1572/1572), 6.40 MiB | 22.77 MiB/s, done.\n",
            "Resolving deltas: 100% (949/949), done.\n",
            "/content/phutball\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF1C55w-O5gg"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbaorYhzwUze",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "# Codebase\n",
        "from lib.model_v2          import TDConway\n",
        "from lib.off_policy        import EpsilonGreedy\n",
        "from lib.optim             import AlternatingTDLambda\n",
        "\n",
        "from lib.training          import training_loop\n",
        "\n",
        "from lib.utilities         import config, lfilter\n",
        "from lib.testing_utilities import create_state, visualize_state, boards\n",
        "from lib.timer             import Timer\n",
        "\n",
        "from lib.move_selection    import get_next_move_training\n",
        "\n",
        "\n",
        "# Graphics for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "plt.ioff()\n",
        "\n",
        "# PyTorch\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOk4kKeOO5gm"
      },
      "source": [
        "## Device Management Utilities\n",
        "Setup for GPU, CPU, or (not working well/fully implemented) TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A7ETHYgbO5gm",
        "outputId": "b67601d3-da71-4f87-d030-c3275ff18649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "  # Install PyTorch/XLA\n",
        "  VERSION = \"nightly\" #[\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  \n",
        "  # Set the device\n",
        "  device = xm.xla_device()\n",
        "  \n",
        "  # Memory inspection\n",
        "  def print_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def print_max_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def garbage_collect():\n",
        "    gc.collect() # No TPU specific implementation yet\n",
        "    \n",
        "elif torch.cuda.is_available():\n",
        "  # Set the device\n",
        "  device = torch.device('cuda')\n",
        "  \n",
        "  # Echo GPU info\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  print(gpu_info)\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage_cuda     as print_memory_usage,\n",
        "    print_max_memory_usage_cuda as print_max_memory_usage,\n",
        "    garbage_collect_cuda        as garbage_collect\n",
        "  )\n",
        "\n",
        "else:\n",
        "  # Set the device to CPU\n",
        "  device = torch.device('cpu')\n",
        "  \n",
        "  # Echo RAM info\n",
        "  from psutil import virtual_memory\n",
        "  from lib.memory import format_bytes\n",
        "  ram = virtual_memory().total\n",
        "  print(format_bytes(ram), 'available memory on CPU-based runtime')\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage, \n",
        "    print_max_memory_usage,\n",
        "    garbage_collect\n",
        "  )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 15 05:25:04 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KxB31wNMO5hG"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tHORM9o9TCgY",
        "colab": {}
      },
      "source": [
        "def save(fname, model, optimizer):\n",
        "  state_dict = {\n",
        "      'model' : model.state_dict(),\n",
        "      'optim' : optimizer.state_dict()\n",
        "  }\n",
        "  torch.save(state_dict, f'{data_dir}/{fname}.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXYUZNU6qQWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fname(version, game_num):\n",
        "  return f'v{version}-{game_num}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WDiYyY9KPXIE"
      },
      "source": [
        "## Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBpw7Ul8wUzs",
        "outputId": "fbd00e6a-d27a-4bd6-a0cc-af662a3b650a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "epsilon_greedy = EpsilonGreedy(0.1)\n",
        "\n",
        "initial_state = create_state('H10').to(device)\n",
        "visualize_state(initial_state)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mlOE_I84O5hD"
      },
      "source": [
        "## Profile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Etz9nnawU0i",
        "outputId": "ae4a809e-620d-4b94-8737-f3536632f245",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%timeit get_next_move_training(initial_state, model, device)\n",
        "%prun training_loop(model, optimizer, 1, device, off_policy = epsilon_greedy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 11.7 ms per loop\n",
            "\n",
            "Playing game 1 of 1:\n",
            "121/121 [==============================] - 4s 30ms/step\n",
            " "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt1GUe13iSVb",
        "colab_type": "text"
      },
      "source": [
        "## Pre-train\n",
        "Teach the model that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTstggDJiTjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lib.pre_training import random_board_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_TWjyDcidf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lib.training import ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJPD_v6GostJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfsZxT94iaIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c649b7bd-0260-4c2f-d9b5-3fdb68c86e5b"
      },
      "source": [
        "loops      = 10000\n",
        "batch_size = 300\n",
        "np.random.seed(42)\n",
        "\n",
        "model = TDConway(config).to(device)\n",
        "optimizer = SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "min_density = 0\n",
        "max_density = 0.3\n",
        "\n",
        "bar = ProgressBar(loops, expandable = False)\n",
        "for _ in range(loops):\n",
        "  boards, targets = random_board_batch(\n",
        "      min_density, \n",
        "      max_density, \n",
        "      batch_size,\n",
        "      device\n",
        "  )\n",
        "\n",
        "  targets.mul_(1/config.cols)\n",
        "\n",
        "  predictions = model(boards, get_all_values = True)\n",
        "  loss        = F.mse_loss(predictions, targets)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  bar.step()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 395s 39ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urTf19bSqUNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = '0.2.1'\n",
        "game_num = 'pre10000x300'\n",
        "\n",
        "save(fname(version, game_num), model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eqLsixqoP3kV"
      },
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsYoeCm2q-Wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TDConway(config).to(device)\n",
        "optimizer = AlternatingTDLambda(model.parameters(), 0.01, 0.9)\n",
        "\n",
        "version = '0.2.1'\n",
        "game_num = 1500\n",
        "\n",
        "sd = torch.load(f'{data_dir}/{fname(version, game_num)}.pt', map_location = device)\n",
        "model.load_state_dict(sd['model']);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rpNxzfZaQyTp",
        "outputId": "d741497f-0f55-4ba9-b2ca-5cfecc4ee58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_loop(model, optimizer, 2000, device, off_policy = epsilon_greedy, verbose = 1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 5944s 3s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n8moRHg1G8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game_num = 3500\n",
        "save(fname(version, game_num), model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lhYwuLNU-d1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eac0784a-8875-4236-d8c6-27be463a9754"
      },
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "eastern = timezone('US/Eastern')\n",
        "print(f'Finished {game_num} games at', datetime.now(eastern).strftime('%I:%M%p %Z'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 3500 games at 04:58AM EDT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgAhTfp0i5z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "421cfe0d-ffa5-4839-977a-9db4b3d9beda"
      },
      "source": [
        "print(f'Finished {game_num} games at', datetime.now(timezone.est).strftime('%I:%M%p %Z'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-8f244b291319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Finished {game_num} games at'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%I:%M%p %Z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.timezone' has no attribute 'est'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92NgliNkT7vm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "6f3709fd-5cef-4e38-8a09-f872ab9b072e"
      },
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "eastern = timezone('US/Eastern')\n",
        "\n",
        "def save(fname, model):\n",
        "  state_dict = {\n",
        "      'model' : model.state_dict(),\n",
        "  }\n",
        "  torch.save(state_dict, f'{data_dir}/{fname}.pt')\n",
        "\n",
        "game_num = 4000\n",
        "version = '0.2.1'\n",
        "\n",
        "batch_size = 500\n",
        "\n",
        "model = TDConway(config).to(device)\n",
        "optimizer = AlternatingTDLambda(model.parameters(), 0.01, 0.9)\n",
        "sd = torch.load(f'{data_dir}/{fname(version, game_num)}.pt', map_location = device)\n",
        "model.load_state_dict(sd['model']);\n",
        "\n",
        "\n",
        "while True: # Until Colab disconnects out of anger\n",
        "  training_loop(model, optimizer, batch_size, device, off_policy = epsilon_greedy, verbose = 1)\n",
        "  game_num += batch_size\n",
        "\n",
        "  save(fname(version, game_num), model)\n",
        "\n",
        "  print(f'Finished {game_num} games at', datetime.now(eastern).strftime('%I:%M%p %Z'))  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 1565s 3s/step\n",
            "Finished 4500 games at 05:53AM EDT\n",
            "500/500 [==============================] - 1539s 3s/step\n",
            "Finished 5000 games at 06:19AM EDT\n",
            "500/500 [==============================] - 1542s 3s/step\n",
            "Finished 5500 games at 06:45AM EDT\n",
            "500/500 [==============================] - 1804s 4s/step\n",
            "Finished 6000 games at 07:15AM EDT\n",
            "500/500 [==============================] - 2266s 5s/step\n",
            "Finished 6500 games at 07:53AM EDT\n",
            "500/500 [==============================] - 2985s 6s/step\n",
            "Finished 7000 games at 08:42AM EDT\n",
            "500/500 [==============================] - 2990s 6s/step\n",
            "Finished 7500 games at 09:32AM EDT\n",
            "500/500 [==============================] - 3032s 6s/step\n",
            "Finished 8000 games at 10:23AM EDT\n",
            "500/500 [==============================] - 2999s 6s/step\n",
            "Finished 8500 games at 11:13AM EDT\n",
            "500/500 [==============================] - 2715s 5s/step\n",
            "Finished 9000 games at 11:58AM EDT\n",
            "500/500 [==============================] - 2933s 6s/step\n",
            "Finished 9500 games at 12:47PM EDT\n",
            "500/500 [==============================] - 2869s 6s/step\n",
            "Finished 10000 games at 01:35PM EDT\n",
            "500/500 [==============================] - 2922s 6s/step\n",
            "Finished 10500 games at 02:24PM EDT\n",
            "112/500 [=====>........................] - ETA: 36:41"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b1708d475e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Until Colab disconnects out of anger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mgame_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/training.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, num_games, device, off_policy, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mgame_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgame_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/training.py\u001b[0m in \u001b[0;36mgame_loop\u001b[0;34m(initial_state, model, optimizer, device, off_policy, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/phutball/pytorch-implementation/lib/optim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, delta, update_trace)\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'trace'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hywC_0PRNKdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Versioning:\n",
        "#  Major versions - major change in approach\n",
        "#  Minor versions - incompatible architecture tweaks\n",
        "#  Build          - retraining or changes in training parameters\n",
        "#  Game number    - number of games trained or pre{E}x{B} where E is the the\n",
        "#                   number of batches and B is the batch size for pre-training\n",
        "# Example: v0.1.2 @400 is the second attempt at training the v0.1 architecture\n",
        "#  and was trained for 400 games\n",
        "\n",
        "# Performance benchmarks.\n",
        "#  GPU benchmarks are on a P100 unless otherwise stated\n",
        "#    per move    : training-relevant\n",
        "#    forward pass: evaluation (arena mode) relevant\n",
        "#  CPU benchmarks are for inference on a fixed set of 300 randomly\n",
        "#    generated boards on an Intel i5 chipset. (deployment-relevant)\n",
        "#  Memory consumption has not been an issue\n",
        "\n",
        "# v0.1: architecture from model_v1. Training: Alternating TD(Î»)\n",
        "#  ~60M params (59,943,809)\n",
        "#  GPU: 100â€“110ms/move with 50-60ms for forward pass\n",
        "#  CPU: 8.1s Â± 0.5s \n",
        "#\n",
        "#  - v0.1.1 - don't use, bug in training\n",
        "#  - v0.1.2 - Use. available @400. Win rate v RandoTron 51% (ðŸ˜¢)\n",
        "\n",
        "# v0.2: architecture from model_v2.\n",
        "#  Training: Alternating TD(Î») WITH pretraining to prefer the ball to the\n",
        "#   right on randomly generated boards\n",
        "#  ~4.4M params (4,381,505)\n",
        "#  GPU: 30-35ms/move with ~12ms for forward pass\n",
        "#  CPU: 1.1s Â± 0.02s\n",
        "#\n",
        "# - v0.2.1 - Available @pre10000x300, @400, @1500, @3500\n",
        "#            Win rate v RandoTron \n",
        "#            - @400: 49%\n",
        "#            - @1500: 56.9% Â±1.8%\n",
        "#            - @3500: 54.8% Â±1.7%\n",
        "#            - In further increments of 500 as [4000, 4500, ..., ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB7XLdBFAI1i",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsHmg68JIkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lib.arena import Player, Battle, RandoTron"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbNXAIuJLS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TDConway(config).to(device)\n",
        "version  = '0.2.1'\n",
        "game_num = 3500\n",
        "sd = torch.load(f'{data_dir}/{fname(version, game_num)}.pt', map_location = device)\n",
        "model.load_state_dict(sd['model'])\n",
        "# optimizer.load_state_dict(sd['optim'])\n",
        "\n",
        "td_conway = Player(model, name = f'TD Conway v{version} @{game_num}')\n",
        "td_conway.eval()\n",
        "randotron = RandoTron()\n",
        "battle    = Battle(td_conway, randotron, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALkfa_V9cjH-",
        "colab_type": "code",
        "outputId": "1decb6fc-3d8d-47cb-ab77-e34d86f973e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "battle.play_match(900, device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900/900 [==============================] - 448s 498ms/step\n",
            "900 games were played between TD Conway v0.2.1 @1500 and RandoTron with 0 draws.\n",
            "The winner was TD Conway v0.2.1 @1500 with a 54.8% win rate!\n",
            "Player 1 on average won in a game of length 95.5.\n",
            "Player 2 on average won in a game of length 84.8\n",
            "Overall average length of game was 90.7\n",
            "Total time taken: 7:28 at\n",
            " - 498ms per finished game.\n",
            " - 5ms per move in a finished game\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}